{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "## Dataset\n",
    "https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "# TODO:\n",
    "### David:\n",
    "- pre-processing\n",
    "  - feature scaling (standardize or normalize ?)\n",
    "  - mapping\n",
    "- feature selection\n",
    "- performance visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "# Environment and sklearn package must be updated to handle at least sklearn version 1.0 for RocCurveDisplay to work\n",
    "# Using anaconda CMD.exe Prompt run \"conda update --all\" to update\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "del sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for the final report.  What is the goal of our final project?\n",
    "\n",
    "Primary\n",
    "\n",
    "* Make RandomForestClassifier work and figure out how to optimize its predictive performance.\n",
    "* Show that it performs better than other algorithms.\n",
    "\n",
    "Secondary\n",
    "\n",
    "* Focus it more on understanding why an ML algorithm performs better\n",
    "* (Overfitting, high correlation, other pitfalls of ML algorithms)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports & Settings\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# display all columns and rows:\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:orange'>NOTE</h1>\n",
    "\n",
    "### Breiman (2001) Section 5:\n",
    "\n",
    "* If, after feature selection, it is found that only a handful of features are actually any good, Breiman seems to suggest combining these features into some additional features for datasets with fewer than 3000 instances.  See paper for implementation.\n",
    "\n",
    "### Section 5.1:\n",
    "\n",
    "* Breiman on handling non-binary categorical variables when forming combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset & Preprocessing\n",
    "\n",
    "# Dataset\n",
    "df_original = pd.read_csv('dataset/telco-customer-churn.csv', index_col=0)\n",
    "df = df_original.copy()\n",
    "\n",
    "# Label Encoding (converting categorical to numerical)\n",
    "categorical_columns = [\n",
    "    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines','InternetService',\n",
    "    'OnlineSecurity', 'OnlineBackup','DeviceProtection','TechSupport','StreamingTV',\n",
    "    'StreamingMovies','Contract','PaperlessBilling','PaymentMethod', 'Churn'\n",
    "]\n",
    "\n",
    "# convert and replace categorical columns with numerical data\n",
    "# encoding is done with categorical labels that sorted alphabetically so df = ['c','z','a'] will always encode to [1,2,0]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[col] = df[col].cat.codes\n",
    "\n",
    "# Dataset where rows with blank TotalCharges ' ' are replaced with 0\n",
    "df_0 = df.copy()\n",
    "df_0['TotalCharges'].replace(\" \", 0, inplace=True)\n",
    "df_0['TotalCharges'] = df_0['TotalCharges'].astype(float)\n",
    "\n",
    "# Dataset where rows with blank TotalCharges ' ' are deleted\n",
    "df_d = df.copy()\n",
    "df_d.drop(df_d[df_d.TotalCharges == \" \"].index, inplace=True)\n",
    "df_d['TotalCharges'] = df_d['TotalCharges'].astype(float)\n",
    "\n",
    "\n",
    "# Dataset where rows with blank TotalCharges ' ' are replaced with the mean of TotalCharges column\n",
    "df_m = df.copy()\n",
    "total_charges_mean = (df_d['TotalCharges'].mean()) # mean from previously calculated dataset\n",
    "df_m['TotalCharges'].replace(\" \", total_charges_mean, inplace=True)\n",
    "df_m['TotalCharges'] = df_m['TotalCharges'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "### Mean decrease in impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCharges        0.197131\n",
      "MonthlyCharges      0.180334\n",
      "tenure              0.151839\n",
      "Contract            0.074690\n",
      "PaymentMethod       0.051708\n",
      "TechSupport         0.045079\n",
      "OnlineSecurity      0.044053\n",
      "gender              0.028285\n",
      "OnlineBackup        0.026994\n",
      "InternetService     0.026422\n",
      "PaperlessBilling    0.025286\n",
      "Partner             0.023297\n",
      "DeviceProtection    0.023238\n",
      "MultipleLines       0.022711\n",
      "SeniorCitizen       0.020900\n",
      "Dependents          0.019474\n",
      "StreamingMovies     0.017063\n",
      "StreamingTV         0.016596\n",
      "PhoneService        0.004899\n",
      "dtype: float64\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/formatters.py\", line 222, in catch_format_error\n",
      "    \"\"\"show traceback on failed format call\"\"\"\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/formatters.py\", line 339, in __call__\n",
      "    pass\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/pylabtools.py\", line 151, in print_figure\n",
      "    fig.canvas.print_figure(bytes_io, **kw)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2319, in print_figure\n",
      "    result = print_method(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 1648, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\", line 412, in wrapper\n",
      "    addendum=(addendum + \" \" + deprecation_addendum) if addendum\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\", line 541, in print_png\n",
      "    mpl.image.imsave(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\", line 1675, in imsave\n",
      "    image.save(fname, **pil_kwargs)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\", line 2184, in save\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\", line 334, in preinit\n",
      "    except ImportError:\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/JpegImagePlugin.py\", line 44, in <module>\n",
      "    from . import Image, ImageFile, TiffImagePlugin\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/TiffImagePlugin.py\", line 52, in <module>\n",
      "    from . import Image, ImageFile, ImageOps, ImagePalette, TiffTags\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/ImageOps.py\", line 240, in <module>\n",
      "    def contain(image, size, method=Image.Resampling.BICUBIC):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\", line 65, in __getattr__\n",
      "    \"LINEAR\": \"BILINEAR\",\n",
      "AttributeError: module 'PIL.Image' has no attribute 'Resampling'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1982, in showtraceback\n",
      "    regular traceback using our InteractiveTB.  In this fashion, apps which\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return None\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    value = undefined\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    tpl_global_var = '%sglobal%s %s%%s%s' % (Colors.em, ColorsNormal,\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    list.append('%s\\n' % stype)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df_m.drop(\"Churn\", axis=1)\n",
    "y = df_m[\"Churn\"]\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X, y)\n",
    "\n",
    "feature_names = X.columns.values.tolist()\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "forest_importances_mdi = pd.Series(importances, index=feature_names)\n",
    "\n",
    "print(forest_importances_mdi.sort_values(ascending=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances_mdi.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI (Mean decrease in impurity)\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x7faa1dd64190 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 359, in __call__\n",
      "    if self.parallel._original_iterator is not None:\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 794, in dispatch_next\n",
      "    against concurrent consumption of the unprotected iterator.\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    # No more tasks available in the iterator: tell caller to stop.\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/reusable_executor.py\", line 177, in submit\n",
      "    return super(_ReusablePoolExecutor, self).submit(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 1115, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "The exit codes of the workers are {SIGKILL(-9), SIGKILL(-9)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n",
      "    for node,mode in to_run:\n",
      "  File \"/var/folders/77/y_32dvyd2xq345ys9v__fvym0000gn/T/ipykernel_69004/642240020.py\", line 3, in <cell line: 3>\n",
      "    result = permutation_importance(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/sklearn/inspection/_permutation_importance.py\", line 259, in permutation_importance\n",
      "    scores = Parallel(n_jobs=n_jobs)(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1056, in __call__\n",
      "    # No need to wait for async callbacks to trigger to\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 935, in retrieve\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 359, in __call__\n",
      "    if self.parallel._original_iterator is not None:\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 794, in dispatch_next\n",
      "    against concurrent consumption of the unprotected iterator.\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    # No more tasks available in the iterator: tell caller to stop.\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 531, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/reusable_executor.py\", line 177, in submit\n",
      "    return super(_ReusablePoolExecutor, self).submit(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 1115, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "The exit codes of the workers are {SIGKILL(-9), SIGKILL(-9)}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1982, in showtraceback\n",
      "    regular traceback using our InteractiveTB.  In this fashion, apps which\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return None\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    value = undefined\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    tpl_global_var = '%sglobal%s %s%%s%s' % (Colors.em, ColorsNormal,\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    list.append('%s\\n' % stype)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/Users/daviddada/opt/anaconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    forest, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "forest_importances_pi = pd.Series(result.importances_mean, index=feature_names)\n",
    "print(forest_importances_pi.sort_values(ascending=False))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances_pi.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:orange'>NOTE</h1>\n",
    "\n",
    "### Breiman (2001) Section 9:\n",
    "\n",
    "* If the input dataset is just a combination of many \"weak\" (low-variance) features, a higher 'max_features' value will perform better so long as the features have low correlation.\n",
    "\n",
    "### Probst (2018) Section 2.3:\n",
    "\n",
    "The formula for Area Under the Curve of a Random Forest for binary classification (either 1 or 0) is:\n",
    "\n",
    "$AUC = \\frac{\\sum_{i=1}^{n_1} \\sum_{j=1}^{n_2} S(\\hat{p}_i^*,\\hat{p}_j^{**})}{n_1n_2}$\n",
    "\n",
    "* $n_1 =$ The number of observations whose class is predicted to be 1\n",
    "* $n_2 =$ The number of observations whose class is predicted to be 0\n",
    "* $\\hat{p}_{i}^*=$ The number of trees that predicted 1 for the $n_1^{th}$ instance divided by the number of trees in the forest\n",
    "* $\\hat{p}_{j}^{**}=$ The number of trees that predicted 1 for the $n_2^{th}$ instance divided by the number of trees in the forest\n",
    "\n",
    "$S(\\hat{p}_i^*,\\hat{p}_j^{**}) =$ A function that returns the following:\n",
    "\n",
    "* If $(\\hat{p}_i^* < \\hat{p}_j^{**})$, return 0.0\n",
    "* If $(\\hat{p}_i^* = \\hat{p}_j^{**})$, return 0.5\n",
    "* If $(\\hat{p}_i^* > \\hat{p}_j^{**})$, return 1.0\n",
    "\n",
    "The larger this value, the better the classifier is.\n",
    "\n",
    "Unsure if this is what's implemented by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "class RFC:\n",
    "    '''\n",
    "    Parameters:\n",
    "      * X = A pandas.DataFrame of features\n",
    "      * y = An associative (same index as X) pandas.Series of labels\n",
    "      * hyperParameters = A dict whose keys correspond to the parameters of\n",
    "        RandomForestClassifer and whose values are lists of arguments for those\n",
    "        parameters\n",
    "    Other notes:\n",
    "      * Research paper references throughout this class are done in this manner:\n",
    "         - Breiman (2001) = B\n",
    "         - Probst  (2018) = P\n",
    "        Section and subsection numbers will be added after these aliases like\n",
    "        B2.1 to show that a comment refers to Breiman (2001) Section 2 Subsection 1\n",
    "    '''\n",
    "    def __init__(self, hyperParameters={}):\n",
    "        hyperParams = \\\n",
    "            self.__getDefaultParams__() if len(hyperParameters) == 0 else self.__validateParams__(hyperParameters)\n",
    "        self.params = self.__getParamCombos__(hyperParams)\n",
    "        self.classifiers = []\n",
    "    \n",
    "    def __getDefaultParams__(self):\n",
    "        ''' Supplies default parameters to test based on what our source papers\n",
    "            suggest are generally good.\n",
    "        '''\n",
    "        out = {}\n",
    "        ''' B4\n",
    "            Suggests selecting more than 1 features to split a node with\n",
    "            doesn't perform any better than selecting 1 so long as we have a high\n",
    "            number of trees.\n",
    "            \n",
    "            sklearn sets its default 'max_features'='sqrt', so this is added as one of\n",
    "            our defaults to investigate why.  Did they find that 'sqrt' performs better\n",
    "            than Breiman's suggested 1?\n",
    "        '''\n",
    "        out['max_features'] = [1, 'sqrt']\n",
    "        ''' P5.3\n",
    "            Suggests that the biggest performance gain is experienced by the first 100\n",
    "            trees.  However, lower 'max_samples' values, lower 'max_depth' values, and\n",
    "            higher 'max_features' values reduce inter-tree correlation and therefore\n",
    "            increase the number of trees needed to reach convergence.\n",
    "        '''\n",
    "        out['max_depth'] = [None, 3]\n",
    "        out['n_estimators'] = [100, 200]\n",
    "        ''' B3\n",
    "            Suggests each bootstrap sample should ideally have 2/3s the length of X.\n",
    "        '''\n",
    "        out['max_samples'] = [float(2/3)]\n",
    "        \n",
    "        out['oob_score'] = [True]\n",
    "        out['n_jobs'] = [-1]\n",
    "        return out\n",
    "\n",
    "    def __validateParams__(self, params):\n",
    "        ''' Removes any misspelled or non-existent-in-RandomForestClassifier\n",
    "            parameter name keys from the dict passed in by the caller.\n",
    "        '''\n",
    "        VALID = ['n_estimators', 'criterion', 'max_depth', 'min_samples_split',\n",
    "                 'min_samples_leaf', 'min_weight_fraction_leaf', 'max_features',\n",
    "                 'max_leaf_nodes', 'min_impurity_decrease', 'boostrap',\n",
    "                 'oob_score','n_jobs', 'random_state', 'verbose', 'warm_start',\n",
    "                 'class_weight', 'ccp_alpha', 'max_samples']\n",
    "        invalidKeys = []\n",
    "        validatedParams = deepcopy(params)\n",
    "        for key in params:\n",
    "            if key not in VALID:\n",
    "                validatedParams.pop(key)\n",
    "                invalidKeys += [key]\n",
    "        if len(invalidKeys) > 0:\n",
    "            print(\"These are not valid RandomForestClassifier parameter names:\\n{}\\n\".format(invalidKeys) + \\\n",
    "                  \"They will not be tuned by GridSearchCV\\n\")\n",
    "        return validatedParams if len(validatedParams) > 0 else self.__getDefaultParams__()\n",
    "    \n",
    "    def __getParamCombos__(self, dictParams):\n",
    "        # Assemble a dict for each unique combination of hyper parameters.  These dicts can be passed into\n",
    "        # a function to supply named arguments\n",
    "        combinations = []\n",
    "        for param, valList in dictParams.items():\n",
    "            if len(combinations) > 0:\n",
    "                for i in range(len(combinations)):\n",
    "                    base = combinations.pop(0)\n",
    "                    for val in valList:\n",
    "                        newCombo = deepcopy(base)\n",
    "                        newCombo[param] = val\n",
    "                        combinations.append(newCombo)\n",
    "            else:\n",
    "                for val in valList:\n",
    "                    combinations.append({param: val})\n",
    "        return combinations\n",
    "    \n",
    "    def train(self, trainX, trainY):\n",
    "        ''' Train an RFC for each unique combination of hyperparameters\n",
    "        Code references:\n",
    "          * Passing named arguments to a function with a dict of names,vals\n",
    "            https://stackoverflow.com/questions/334655/passing-a-dictionary-to-a-function-as-keyword-parameters\n",
    "        '''\n",
    "        print(\"Training {} classifiers...\".format(len(self.params)))\n",
    "        for dictParams in self.params:\n",
    "            print(\"\\t{}\".format(dictParams))\n",
    "            newRfc = RandomForestClassifier(**dictParams)\n",
    "            newRfc.fit(trainX, trainY)\n",
    "            self.classifiers.append(newRfc)\n",
    "            \n",
    "    def showRocAuc(self, testX, testY):\n",
    "        if len(self.classifiers) == 0:\n",
    "            raise AttributeError(\"No classifiers have been trained yet.  You must call .train() to create \" + \\\n",
    "                                 \"some classifiers to display.\")\n",
    "\n",
    "        fig = plt.figure(dpi=300)\n",
    "        axes = plt.gca()\n",
    "        for i in range(len(self.classifiers)):\n",
    "            skm.RocCurveDisplay.from_estimator(\n",
    "                self.classifiers[i],\n",
    "                testX,\n",
    "                testY,\n",
    "                pos_label=0,\n",
    "                name=\"RFC #{}\".format(i+1),\n",
    "                ax=axes)\n",
    "\n",
    "\n",
    "## Testing non-default hyper parameters\n",
    "X_train, X_test, y_train, y_test = split(df_m.drop(\"Churn\", axis=1),  df_m[\"Churn\"], test_size=0.1)\n",
    "rfc = RFC(hyperParameters={\n",
    "    'max_features': [1],\n",
    "    'max_depth': [3],\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_samples': [float(2/3)],\n",
    "    'oob_score': [True],\n",
    "    'n_jobs': [-1]\n",
    "})\n",
    "rfc.train(X_train, y_train)\n",
    "rfc.showRocAuc(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics\n",
    "\n",
    "# Get predictions\n",
    "y_pred = rfc.classifiers[0].predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cf = skm.confusion_matrix(y_pred, y_test)\n",
    "sns.heatmap(cf/np.sum(cf), fmt='.2%', annot=True, cmap='Blues')\n",
    "\n",
    "# Classification Report\n",
    "print(rfc.params[0], end=\"\\n\\n\")\n",
    "print(skm.classification_report(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
